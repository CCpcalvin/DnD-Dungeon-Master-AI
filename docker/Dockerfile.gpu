# docker/Dockerfile.gpu
# Use CUDA 12.3 base image
FROM nvidia/cuda:12.3.0-base-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV LLAMA_CUBLAS=1

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    git \
    ninja-build \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copy requirements first to leverage Docker cache
COPY requirements.txt .

# Install Python dependencies with CUDA support
# Note: Adjust the extra index URL based on your CUDA version
RUN pip install --no-cache-dir -r requirements.txt --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123

# Setup IPython config
RUN mkdir -p /root/.ipython/profile_default/ && \
    echo "c = get_config()\n\
c.InteractiveShellApp.extensions = ['autoreload']\n\
c.InteractiveShellApp.exec_lines = [\n\
    '%autoreload 2',\n\
    'print(\"Autoreload is active. Changes to modules will be auto-reloaded.\")'\n\
]" > /root/.ipython/profile_default/ipython_config.py

# Set default command
CMD ["python", "dnd_draft/llama.py"]